{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8VlWwM8JjX6o"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyD-5TO2w4u9ljArT7vuaJjyHcAtTA3eZLw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_OX8w_WZlYiz"
   },
   "outputs": [],
   "source": [
    "!pip install -q youtube-transcript-api langchain langchain-community \\\n",
    "langchain-google-genai google-generativeai chromadb faiss-cpu tiktoken python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gVTI2iKellRh"
   },
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi,TranscriptsDisabled\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 998,
     "status": "ok",
     "timestamp": 1767172127416,
     "user": {
      "displayName": "LALIT MAHAJAN",
      "userId": "12467921337685661721"
     },
     "user_tz": -330
    },
    "id": "X8MynIm5kmN2",
    "outputId": "485d9b32-105b-4946-c2da-3bb36361e261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you are signing up for a new app, you enter your preferred username and get a message saying, \"This username is already taken.\" It feels like a small inconvenience, but behind the scenes, that simple check is surprisingly complex. When you're dealing with billions of users, checking whether a username exist can't rely on a basic database query. That would create serious performance issues, high latency, bottlenecks, and unnecessary load in the system. So, how do large scale platforms like Google, Amazon, and Meta solve this? In this video, we'll walk through the smart techniques they use from in-memory data structures like Bloom filters and hashts to distributed databases like Cassandra and Dynamob. We'll also see how caching layers and load balancing work together to make these checks incredibly fast and scalable. So let's get [Music] started to understand how these systems achieve such speed. Let's start with a radius hashmap. A powerful and efficient data structure often used in caching layers. In radius, a hashmap lets you store multiple field value pairs under one key. For username lookups, each field can represent a username and its value could be something lightweight like a user ID or even a placeholder flag. When a user checks if a username is available, the system queries this hashmap. If the field exists, that is if the username is already in the map, that's a cache hit and radius returns a result instantly. It's a fast in-memory check that avoids touching the database for the vast majority of lookups. But of course, you can't store billions of usernames in a single radius instance forever. Memory is finite. Regardless, radius hashmaps work brilliantly for exact match lookups. If the username exist, we get an answer instantly. But what if we want more than just a yes or no? What if we want to suggest similar available usernames or check all names that start with a given prefix? And that's where tries or prefix trees come into play. A try is a treel like structure that organizes strings by their shared prefixes. So instead of storing each username as a whole, it breaks them down character by character and builds a path through the tree. This allows us to perform lookups in O of M time where M is the length of the string no matter how many total usernames we have. So even if there are billions of entries, checking a single username like bitemong takes time proportional to its length, not the size of the entire data set. And that's not all. Tries naturally support prefix based queries and autocomplete, which makes them ideal for suggesting usernames when a user's first choice is already taken. The space saving trick is that usernames with shared prefixes reuse the same path. For example, bite monk and bitem io username share the same bitemunk branch reducing redundancy. However, tries do have their limitations. They can consume a lot of memory especially if there is isn't much overlap between usernames. So to make them more efficient, systems often use compressed tries like rad xrays or limit their use to hard data that is frequently checked or recently entered usernames. Now, while tries are great for prefix based lookups and autocomplete, when it comes to storing and searching large sorted data sets, especially in traditional databases, another powerful structure comes into play. The B+ tree. B+ tree and their close cousin B trees are widely used in relational databases to index fields like usernames. These structures keep keys sorted and allow efficient lookups in o of login time. So even with the billion usernames, finding one might take around only 30 steps. Thanks to their high fan out, meaning each node can store hundreds of keys, the tree stays shallow. In real world scenarios, you can often search millions of entries with just three to four disk or memory reads. B+ trees also support range queries like finding the next available username alphabetically, something that hashmaps and bloom filters can do. that makes them ideal for scenarios where ordering matters and you'll find B+3s under the hood of many databases from traditional SQL systems to modern NoSQL ones like MongoDB and Foundation DB. However, as the data set grows into the billions, maintaining B+3's performance on a single machine becomes difficult and that's where systems like Google Cloud Spanner shine. Spanner distributes a sorted key space backed by B3 like structures across multiple machines and replicas. This allows it to scale horizontally while still supporting millions of queries per second with low latency. The key strength of B+ is that they provide exact lookups with no false positives while also enabling ordered scans. Their main trade-off managing the complexity of update and distribution across nodes in massive scale environments. Still they remain a core indexing mechanism in large scale username systems especially when built on top of relational or NoSQL backends. Now we have looked at radius hashmaps for speed drives for prefix matching and B+3s for sorted lookups. But what if we want something that's lightning fast memory efficient and doesn't even need to store the actual usernames? And that's where bloom filters come in. If you have seen my earlier video on bloom filters, you will remember that they are clever proistic data structure designed for one thing, checking if an item might be in a set using as little memory as possible. And here is how they work. A bloom filter is just a bit array combined with handful of hash functions. When you add a username, it's hashed multiple times and each hash sets a corresponding bit in the array. To check a username, you hash it the same way. And if any of those bits is zero, the username is definitely not in the set. If all the bits are one, then it's probably present. And that's when you fall back to a more expensive check like a database query. What makes Bloom filters so powerful is that they never give false negatives. If they say the username isn't present, you can trust that. The only trade-off is that possibility of false positives, but that's usually acceptable when the alternative is scoring a massive database. And the space savings are significant. To store 1 billion usernames with a 1% false positive rate, you would need roughly around 1.2 GB of memory. That's a fraction of what it would take to store full keys in a cache. This is why large scale systems like Cassandra, for example, use Bloom filters to avoid unnecessary disc lookups. And in some cases, companies keep a global bloom filter of all taken usernames in memory. So most lookups can be filtered up front, saving both time and compute. In short, Bloom filters act as a first line of defense. They catch the definitely not present cases instantly, reducing load on caches and database downstream. And that's what makes them a go-to technique in systems that deal with billions of records and need fast, cost-effective membership checks. And here is a clear comparison table of the data structures we have discussed. Red is hashmap, try, b+3s and bloom filter highlighting their performance, memory usage and best use cases in large scale username lookups. So far we have explored the individual building blocks from hashmap to tries B+3s and bloom filters each with their own strengths and trade-offs. But in real world large scale systems it's rarely about picking just one. Instead, companies like Google, Facebook, and Amazon combine these data structures strategically, layering them to maximize speed, reducing memory usage, and minimizing database load. Let's look at how these components come together in practice and how tech giants architect their systems to handle billions of username lookups efficiently. Imagine you are checking if username bite monk is available. First, a load balancer routes the request. Now, in large distributed systems, load balancing typically happens at two levels. global and local. Global load balancing or edge level load balancing uses DNS-based or anycast routing to direct user request to the closest regional data center. For example, a user in Europe gets routed to an EU based data center rather than one in the US. In AWS, the global load balancer is typically handled by Amazon Route 53. Inside each data center, a local load balancer like EngineX or AWS ELB distributes traffic among several backend servers or service instances. These backend servers run application logic, including the Bloom filter and cache queries. A Bloom filter is typically not a standalone server, but rather a small fast in-memory data structure that sits within your application servers or query servers. Usually, each backend application server maintains a copy of the Bloom filter in memory. synchronized periodically from a central source or rebuilt regularly from the database. So instead of going straight to the database, your query hits a bloom filter. Think of Bloom filter as the bouncer at a club. Super quick but occasionally mistaken. It instantly tells you if the username definitely doesn't exist, allowing you to avoid slow database checks altogether. If the Bloom filter isn't sure, your query moves on. Next, your request hits a lightning fast in-memory cache, often radius or memcacheed. This cache is like a destroyer. Everything you recently used is closed at hand. And so, if bite monk was checked recently, your answer comes back in microsconds. But what if the cache doesn't have it? Then it's a cache miss and only then you query the actual distributed database. Big tech often chooses databases designed specifically for scale like Apache Cassandra at Instagram or Amazon Dynamo TV. These databases cleverly split data across hundreds of thousands of machines using strategies like consistent hashing. This way the load gets evenly distributed and lookups happen incredibly fast. The authoritative check happens here and responds definitively on username existence. Application server then sends the final response back through load balancers to the user. By combining quick probabistic filters, memory caches, distributed databases, and smart load balancing. Companies like Google, Facebook and Amazon ensure you instantly know if your chosen username is taken, even among billions of users worldwide. So the next time you see username already taken, remember there's a whole system working behind the scenes to make that check lightning fast and globally scalable. From Bloom filters and radius caches to distributed databases like Cassandra and Spanner, it's a beautiful blend of computer science and real world engineering. And if you found this breakdown helpful, hit that like button, subscribe for more deep dives into system design and architecture, and let me know in comments what you would like to explore next. Thanks for watching, and I'll see you in the next one. [Music] \n"
     ]
    }
   ],
   "source": [
    "video_id = \"_l5Q5kKHtR8\"\n",
    "\n",
    "try:\n",
    "  transcript_list = YouTubeTranscriptApi().fetch(video_id)\n",
    "  video_transcript=\"\"\n",
    "  for transcript in transcript_list:\n",
    "    video_transcript += transcript.text + \" \"\n",
    "  print(video_transcript)\n",
    "\n",
    "except TranscriptsDisabled:\n",
    "  print(\"No caption available for this video\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1767172238772,
     "user": {
      "displayName": "LALIT MAHAJAN",
      "userId": "12467921337685661721"
     },
     "user_tz": -330
    },
    "id": "EjOYAzDwoH5F",
    "outputId": "5b4ee240-5535-4f35-b1d7-8278da277bb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "chunks = splitter.create_documents([video_transcript])\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-sHBJI5rGxO"
   },
   "outputs": [],
   "source": [
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "vector_store = FAISS.from_documents(chunks,embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1767172381477,
     "user": {
      "displayName": "LALIT MAHAJAN",
      "userId": "12467921337685661721"
     },
     "user_tz": -330
    },
    "id": "aNPo7jrSrkUx",
    "outputId": "561f673a-d04d-495f-d8fb-f4c52e6577a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'f5f546a5-3fa9-4f04-bfcd-0912cdf1422c',\n",
       " 1: '40598a12-5c8e-4df6-b760-ec33eb16c7d5',\n",
       " 2: '25ed4831-f62c-4b52-b27b-0a44d1cd6e34',\n",
       " 3: '4c412e96-b101-4ce9-8d23-0c2a448838b1',\n",
       " 4: 'ef3b9b7c-5f3b-46f8-bec9-e987caf6dee1',\n",
       " 5: '1e3a5b89-dc69-4d6a-a949-71eef1c604cf',\n",
       " 6: '8488e278-5148-4048-a4d2-f311e6dd0282',\n",
       " 7: 'a1ec1853-d6d8-489f-abd7-a2fbc5ed8eb5',\n",
       " 8: '4797a2ee-3de1-4e82-833e-fc41aa70e55c',\n",
       " 9: '4712adac-28bd-4b19-af73-e5932b5cdce7',\n",
       " 10: '696266ee-1a58-412a-a326-ac432c80abcf',\n",
       " 11: '98b6c4b4-c92c-47fa-91bc-8d23b7b60ee5',\n",
       " 12: '80547320-7bce-42fa-af0c-9b31c74085e4',\n",
       " 13: '7632135a-e95d-4747-a50a-9ed013a950aa'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1767172424372,
     "user": {
      "displayName": "LALIT MAHAJAN",
      "userId": "12467921337685661721"
     },
     "user_tz": -330
    },
    "id": "9Pj5GuocrpmZ",
    "outputId": "4de2c188-53a3-4cc7-cf9e-ab047272c7a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='ef3b9b7c-5f3b-46f8-bec9-e987caf6dee1', metadata={}, page_content=\"when it comes to storing and searching large sorted data sets, especially in traditional databases, another powerful structure comes into play. The B+ tree. B+ tree and their close cousin B trees are widely used in relational databases to index fields like usernames. These structures keep keys sorted and allow efficient lookups in o of login time. So even with the billion usernames, finding one might take around only 30 steps. Thanks to their high fan out, meaning each node can store hundreds of keys, the tree stays shallow. In real world scenarios, you can often search millions of entries with just three to four disk or memory reads. B+ trees also support range queries like finding the next available username alphabetically, something that hashmaps and bloom filters can do. that makes them ideal for scenarios where ordering matters and you'll find B+3s under the hood of many databases from traditional SQL systems to modern NoSQL ones like MongoDB and Foundation DB. However, as the\")]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.get_by_ids(['ef3b9b7c-5f3b-46f8-bec9-e987caf6dee1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2lIOHZnr0E9"
   },
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1767172532392,
     "user": {
      "displayName": "LALIT MAHAJAN",
      "userId": "12467921337685661721"
     },
     "user_tz": -330
    },
    "id": "IVVRi6LnsMxo",
    "outputId": "ec01e1fb-6453-4841-bfaa-bccc33d379c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7c0c1dee13a0>, search_kwargs={'k': 4})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1767172585425,
     "user": {
      "displayName": "LALIT MAHAJAN",
      "userId": "12467921337685661721"
     },
     "user_tz": -330
    },
    "id": "Hv5wcv2LsOdI",
    "outputId": "4fe40afc-9668-4535-a194-cc38502ae51d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='ef3b9b7c-5f3b-46f8-bec9-e987caf6dee1', metadata={}, page_content=\"when it comes to storing and searching large sorted data sets, especially in traditional databases, another powerful structure comes into play. The B+ tree. B+ tree and their close cousin B trees are widely used in relational databases to index fields like usernames. These structures keep keys sorted and allow efficient lookups in o of login time. So even with the billion usernames, finding one might take around only 30 steps. Thanks to their high fan out, meaning each node can store hundreds of keys, the tree stays shallow. In real world scenarios, you can often search millions of entries with just three to four disk or memory reads. B+ trees also support range queries like finding the next available username alphabetically, something that hashmaps and bloom filters can do. that makes them ideal for scenarios where ordering matters and you'll find B+3s under the hood of many databases from traditional SQL systems to modern NoSQL ones like MongoDB and Foundation DB. However, as the\"),\n",
       " Document(id='1e3a5b89-dc69-4d6a-a949-71eef1c604cf', metadata={}, page_content=\"them ideal for scenarios where ordering matters and you'll find B+3s under the hood of many databases from traditional SQL systems to modern NoSQL ones like MongoDB and Foundation DB. However, as the data set grows into the billions, maintaining B+3's performance on a single machine becomes difficult and that's where systems like Google Cloud Spanner shine. Spanner distributes a sorted key space backed by B3 like structures across multiple machines and replicas. This allows it to scale horizontally while still supporting millions of queries per second with low latency. The key strength of B+ is that they provide exact lookups with no false positives while also enabling ordered scans. Their main trade-off managing the complexity of update and distribution across nodes in massive scale environments. Still they remain a core indexing mechanism in large scale username systems especially when built on top of relational or NoSQL backends. Now we have looked at radius hashmaps for speed\"),\n",
       " Document(id='4c412e96-b101-4ce9-8d23-0c2a448838b1', metadata={}, page_content=\"takes time proportional to its length, not the size of the entire data set. And that's not all. Tries naturally support prefix based queries and autocomplete, which makes them ideal for suggesting usernames when a user's first choice is already taken. The space saving trick is that usernames with shared prefixes reuse the same path. For example, bite monk and bitem io username share the same bitemunk branch reducing redundancy. However, tries do have their limitations. They can consume a lot of memory especially if there is isn't much overlap between usernames. So to make them more efficient, systems often use compressed tries like rad xrays or limit their use to hard data that is frequently checked or recently entered usernames. Now, while tries are great for prefix based lookups and autocomplete, when it comes to storing and searching large sorted data sets, especially in traditional databases, another powerful structure comes into play. The B+ tree. B+ tree and their close cousin B\"),\n",
       " Document(id='98b6c4b4-c92c-47fa-91bc-8d23b7b60ee5', metadata={}, page_content=\"or rebuilt regularly from the database. So instead of going straight to the database, your query hits a bloom filter. Think of Bloom filter as the bouncer at a club. Super quick but occasionally mistaken. It instantly tells you if the username definitely doesn't exist, allowing you to avoid slow database checks altogether. If the Bloom filter isn't sure, your query moves on. Next, your request hits a lightning fast in-memory cache, often radius or memcacheed. This cache is like a destroyer. Everything you recently used is closed at hand. And so, if bite monk was checked recently, your answer comes back in microsconds. But what if the cache doesn't have it? Then it's a cache miss and only then you query the actual distributed database. Big tech often chooses databases designed specifically for scale like Apache Cassandra at Instagram or Amazon Dynamo TV. These databases cleverly split data across hundreds of thousands of machines using strategies like consistent hashing. This way the\")]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"How b+ tree works in huge database ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixh-QkfdsbUt"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model='models/gemini-2.5-flash',temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEJXh9jhtkMI"
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template = '''\n",
    "    you are a helpful assitant\n",
    "    answer only from the provide transcirpt context\n",
    "    if the contes is insufficient , just say i don't know\n",
    "\n",
    "    {context}\n",
    "    Question:{question}\n",
    "    ''',\n",
    "    input_variables = ['context','question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWQ6wOSduB1o"
   },
   "outputs": [],
   "source": [
    "question = \"what is two pointer approach ?\"\n",
    "retrieved_docs = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_ssDqYKuc3K"
   },
   "outputs": [],
   "source": [
    "context = \"\\n\\n\".join(docs.page_content for docs in retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhNWTJG9uyfE"
   },
   "outputs": [],
   "source": [
    "final_prompt = prompt.invoke({\"context\":context , \"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1462,
     "status": "ok",
     "timestamp": 1767173360275,
     "user": {
      "displayName": "LALIT MAHAJAN",
      "userId": "12467921337685661721"
     },
     "user_tz": -330
    },
    "id": "gJA3L3EevFgD",
    "outputId": "d3eaa25e-0973-4e4d-b194-5c7c79331f07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know\n"
     ]
    }
   ],
   "source": [
    "answer = llm.invoke(final_prompt)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "executionInfo": {
     "elapsed": 72771,
     "status": "error",
     "timestamp": 1767175655927,
     "user": {
      "displayName": "LALIT MAHAJAN",
      "userId": "12467921337685661721"
     },
     "user_tz": -330
    },
    "id": "ScbVgEvEvL7F",
    "outputId": "0ca8477c-7b2e-4105-9118-0db4e4786b59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter YouTube URL : https://www.youtube.com/shorts/NQoF0knFy7U\n",
      "User : what is redis?\n",
      "\n",
      "RAG_BOT :  I don't know \n",
      "\n",
      "User : summarize the video\n",
      "\n",
      "RAG_BOT :  Reddis is extremely fast because all its data is stored in memory, specifically in RAM (Random Access Memory), instead of on a disk like traditional databases. RAM is significantly faster for reading and writing data compared to disk storage. RAM has an access time of around 120 nanoseconds, which is much faster than SSDs or HDDs that operate in the microsecond to millisecond range. This leverage of RAM allows Reddis to deliver ultra-low latency performance. \n",
      "\n",
      "User : what is reds?\n",
      "\n",
      "RAG_BOT :  I don't know. \n",
      "\n",
      "User : how redis works\n",
      "\n",
      "RAG_BOT :  Redis stores all its data in memory (RAM) instead of on disk. This means that all reading and writing of data are completed in memory. Leveraging RAM allows Redis to deliver ultra-low latency performance because RAM is much faster to read from and write to compared to disk storage. \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3547534613.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mmain_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_chain\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi,TranscriptsDisabled\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnableBranch,RunnableSequence ,RunnablePassthrough, RunnableParallel,RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import re\n",
    "#prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template = '''\n",
    "    you are a helpful assitant\n",
    "    answer only from the provide transcirpt context\n",
    "    if the contes is insufficient , just say i don't know\n",
    "\n",
    "    {context}\n",
    "    Question:{question}\n",
    "    ''',\n",
    "    input_variables = ['context','question']\n",
    ")\n",
    "\n",
    "#output parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "#Chat Model\n",
    "llm = ChatGoogleGenerativeAI(model='models/gemini-2.5-flash',temperature=0.2)\n",
    "\n",
    "#embedding Model\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "#url to id fetcher\n",
    "\n",
    "\n",
    "def extract_video_id(url: str):\n",
    "    \"\"\"\n",
    "    Supports:\n",
    "    - https://www.youtube.com/watch?v=VIDEO_ID\n",
    "    - https://youtu.be/VIDEO_ID\n",
    "    - https://www.youtube.com/embed/VIDEO_ID\n",
    "    \"\"\"\n",
    "    pattern = r\"(?:v=|\\/)([0-9A-Za-z_-]{11})\"\n",
    "    match = re.search(pattern, url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid YouTube URL\")\n",
    "\n",
    "#taking video id and getting transcript\n",
    "yt_url = input(\"Enter YouTube URL : \")\n",
    "\n",
    "try:\n",
    "    video_id = extract_video_id(yt_url)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "  transcript_list = YouTubeTranscriptApi().fetch(video_id)\n",
    "  video_transcript=\"\"\n",
    "  for transcript in transcript_list:\n",
    "    video_transcript += transcript.text + \" \"\n",
    "except TranscriptsDisabled:\n",
    "  print(\"No caption available for this video\")\n",
    "\n",
    "#spliting whole transcript into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "chunks = splitter.create_documents([video_transcript])\n",
    "\n",
    "#storing the chunks in vector database\n",
    "vector_store = FAISS.from_documents(chunks,embedding_model)\n",
    "\n",
    "#applying retriever to find the similar content from the transcript\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":4})\n",
    "\n",
    "#lambda function which will make one string of 4 documents\n",
    "def format_docs(retrieved_docs):\n",
    "  context = \"\\n\\n\".join(docs.page_content for docs in retrieved_docs)\n",
    "  return context\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'context':retriever | RunnableLambda(format_docs),\n",
    "    'question':RunnablePassthrough()\n",
    "})\n",
    "\n",
    "while True:\n",
    "    question = input(\"User : \")\n",
    "    main_chain = parallel_chain | prompt | llm | parser\n",
    "    answer = main_chain.invoke(question)\n",
    "    print(\"\\nRAG_BOT : \",answer,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLPk3bAKxqkH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN3DRsGFT4KV+pDXE8X91bj",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
